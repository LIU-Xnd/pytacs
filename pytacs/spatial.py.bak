from scanpy import AnnData as _AnnData
import numpy as _np
from numpy.typing import NDArray
from scipy.sparse import csr_matrix as _csr_matrix
from scipy.spatial import distance_matrix as _distance_matrix
from .classifier import _LocalClassifier
from .utils import radial_basis_function as _rbf
from .utils import to_array as _to_array
from .utils import _UNDEFINED, _Undefined


class SpatialHandler:
    """A spatial handler to produce filtrations, integrate spots into single cells,
    and estimate their confidences.

    Args:
        adata_spatial (AnnData): subcellular spatial transcriptomic
        AnnData, like Stereo-seq, with .obs[['x', 'y']] indicating
        the locations of spots.

        local_classifier (LocalClassifier): a trained local classifier
        on reference scRNA data.

        threshold_adjacent (float): spots within this distance are
        considered adjacent. for integer-indexed spots, 1.2 for
        4-neighbor adjacency and 1.5 for 8-neighbor adjacency.

        max_spots_per_cell (int): max number of spots of a single
        cell.

        scale_rbf (float): next spot to add is selected from adjacent
        spots, with a radial basis function probability, whose scale
        factor is this parameter."""

    def __init__(
        self,
        adata_spatial: _AnnData,
        local_classifier: _LocalClassifier,
        threshold_adjacent: float = 1.2,
        max_spots_per_cell: int = 81,
        scale_rbf: float = 1.0,
    ):
        assert _np.all(
            adata_spatial.obs.index.astype(_np.int_)
            == _np.arange(adata_spatial.shape[0])
        ), "Spatial AnnData needs tidying using AnnDataPreparer!"
        self.adata_spatial = adata_spatial
        self.threshold_adjacent = threshold_adjacent
        self.local_classifier = local_classifier

        self.max_spots_per_cell = max_spots_per_cell
        self.scale_rbf = scale_rbf

        self._filtrations: dict[int, list[int]] = dict()
        # dict {idx_centroid: [idx_centroid, idx_spot_level1, idx_spot_level2, ...]}

        self._mask_newIds = _np.full(
            (self.adata_spatial.X.shape[0],), fill_value=-1, dtype=int
        )
        # mask on each old sample. -1 for not assigned; otherwise the new id.

        self._classes_new: dict[int, int] = dict()
        # new id -> new class

        self._confidences_new: dict[int, float] = dict()
        # new id -> confidence

        self.cache_distance_matrix: NDArray[_np.float_] | _Undefined = _UNDEFINED
        self.cache_singleCellAnnData: _AnnData | _Undefined = _UNDEFINED
        return None

    @property
    def threshold_confidence(self) -> float:
        return self.local_classifier.threshold_confidence

    @property
    def has_negative_control(self) -> bool:
        return self.local_classifier.has_negative_control

    @property
    def filtrations(self) -> dict:
        copy_ = dict()
        for k, v in self._filtrations.items():
            copy_[k] = v.copy()
        return copy_

    @property
    def mask_newIds(self) -> NDArray[_np.int_]:
        return self._mask_newIds.copy()

    @property
    def masked_spotIds(self) -> NDArray[_np.int_]:
        """Already positively masked spot ids."""
        return _np.where(self.mask_newIds > -1)[0]

    @property
    def unmasked_spotIds(self) -> NDArray[_np.int_]:
        return _np.where(self.mask_newIds == -1)[0]

    @property
    def sampleIds_new(self) -> NDArray[_np.int_]:
        return _np.sort(_np.array(list(set(list(_np.unique(self.mask_newIds))) - {-1})))

    @property
    def classes_new(self) -> dict:
        return self._classes_new.copy()

    @property
    def confidences_new(self) -> dict:
        return self._confidences_new.copy()

    def __repr__(self) -> str:
        return f"""--- Spatial Handler (pytacs) ---
- adata_spatial: {self.adata_spatial}
- threshold_adjacent: {self.threshold_adjacent}
- local_classifier: {self.local_classifier}
    + threshold_confidence: {self.threshold_confidence}
    + has_negative_control: {self.has_negative_control}
- max_spots_per_cell: {self.max_spots_per_cell}
- scale_rbf: {self.scale_rbf}
- filtrations: {len(self.filtrations)} fitted
- single-cell segmentation:
    + new samples: {len(self.sampleIds_new)}
    + AnnData: {self.cache_singleCellAnnData}
--- --- --- --- --- ---
"""

    def clear_cache(self) -> None:
        self.cache_distance_matrix = _UNDEFINED
        self.cache_singleCellAnnData = _UNDEFINED
        return None

    def _compute_distance_matrix(self):
        points = self.adata_spatial.obs[["x", "y"]].values
        self.cache_distance_matrix = _distance_matrix(points, points)
        return None

    def _find_adjacentOfOneSpot_spotIds(self, idx_this_spot: int) -> NDArray[_np.int_]:
        """Find all adjacent spots, including self."""
        if self.cache_distance_matrix is _UNDEFINED:
            self._compute_distance_matrix()
        assert type(self.cache_distance_matrix) is _np.ndarray
        distances = self.cache_distance_matrix[idx_this_spot, :]
        idxs_adjacent = _np.where(distances <= self.threshold_adjacent)[0]
        return idxs_adjacent

    def _find_adjacentOfManySpots_spotIds(
        self, filtration: list[int]
    ) -> NDArray[_np.int_]:
        """Find all adjacent spots of a list of indices of spots (called filtration),
        excluding selves and already positively masked spots."""
        assert len(filtration) > 0
        adj_spots = []
        for spot in filtration:
            adj_spots += list(self._find_adjacentOfOneSpot_spotIds(spot))
        return _np.array(
            list((set(adj_spots) - set(filtration)) -
                 set(list(self.masked_spotIds)))
        )

    def _buildFiltration_addOneSpot(
        self,
        idx_centroid: int,
        verbose: bool = True,
    ) -> int:
        """Randomly (with rbf probs) adds one spot for the cell
         centered at idx_centroid.

        Filtration list includes idx_centroid itself.

        Update the self.filtrations,
         and returns the added spot index (-1 for not added)."""
        loc_centroid = self.adata_spatial.obs[[
            "x", "y"]].values[idx_centroid, :]
        self._filtrations[idx_centroid] = self._filtrations.get(
            idx_centroid, [idx_centroid]
        )
        if len(self._filtrations[idx_centroid]) >= self.max_spots_per_cell:
            if verbose:
                print(
                    f"Warning: reaches max_spots_per_cell {
                        self.max_spots_per_cell}"
                )
            return -1
        # Find adjacent candidate spots
        idxs_adjacent = self._find_adjacentOfManySpots_spotIds(
            self._filtrations[idx_centroid]
        )
        if len(idxs_adjacent) == 0:
            if verbose:
                print(
                    f"Warning: no adjacent spots found! Check threshold_adjacent {
                        self.threshold_adjacent}"
                )
            return -1
        # Calculate the probs
        probs_: list[float] = []
        for idx in idxs_adjacent:
            loc_adj = self.adata_spatial.obs[["x", "y"]].values[idx, :]
            probs_.append(_rbf(loc_adj, loc_centroid, scale=self.scale_rbf))
        probs: NDArray[_np.float_] = _np.array(probs_)
        probs /= _np.sum(probs_)
        # Select one randomly
        idx_selected = _np.random.choice(idxs_adjacent, p=probs)
        # Update the filtration
        self._filtrations[idx_centroid].append(idx_selected)
        return idx_selected

    def _aggregate_spots_given_filtration(
        self,
        filtration: list[int],
    ) -> NDArray:
        """Returns a 1d-array of counts of genes."""
        idxs_filtration = _np.array(filtration)
        if type(self.adata_spatial.X) is NDArray:
            return self.adata_spatial.X[idxs_filtration, :].sum(axis=0)
        return self.adata_spatial.X.toarray()[idxs_filtration, :].sum(axis=0)

    def _compute_confidence_of_level(
        self,
        idx_centroid: int,
        level: int | None = None,
    ) -> NDArray[_np.float_]:
        """Calculate confidence of filtrations[idx_centroid][:level+1]"""
        self._filtrations[idx_centroid] = self._filtrations.get(
            idx_centroid, [idx_centroid]
        )
        if level is None:
            filtration_this_level = self.filtrations[idx_centroid][:]
        else:
            filtration_this_level = self.filtrations[idx_centroid][: level + 1]
        probas = self.local_classifier.predict_proba(
            X=_np.array(
                [list(self._aggregate_spots_given_filtration(filtration_this_level))]
            ),
            genes=self.adata_spatial.var.index,
        )[0, :]
        # Remove the NegativeControl proba
        # print(probas)
        if self.has_negative_control:
            probas = probas[:-1]
        return probas

    def _buildFiltration_addSpotsUntilConfident(
        self,
        idx_centroid: int,
        n_spots_add_per_step: int = 1,
        verbose: bool = True,
    ) -> tuple[float, int, int]:
        """Find many spots centered at idx_centroid that are confidently in a class.

        Update the self.filtrations, update the self.mask_newIds, self.confidences_new,
         self.classes_new,
         and returns the (confidence, class_id, new_sampleId).
         If reaches max_spots_per_cell and still not confident, returns the
         (confidence, -1, and idx_centroid)."""
        label: int = -1
        confidence: float = 0.0
        for _ in range(self.max_spots_per_cell):
            probas = self._compute_confidence_of_level(
                idx_centroid, level=None)
            label = int(_np.argmax(probas))
            confidence = probas[label]
            if confidence >= self.threshold_confidence:
                self._mask_newIds[_np.array(self.filtrations[idx_centroid])] = (
                    idx_centroid
                )
                self._classes_new[idx_centroid] = label
                self._confidences_new[idx_centroid] = confidence
                break
            # Add n cells per step.
            for i_add in range(n_spots_add_per_step):
                idx_added = self._buildFiltration_addOneSpot(
                    idx_centroid, verbose=verbose
                )
                if idx_added == -1:
                    break
            if idx_added == -1:
                label = -1
                break
        else:
            label = -1
        if label == -1:
            # Clear filtrations that are not confident
            del self._filtrations[idx_centroid]
        return (confidence, label, idx_centroid)

    # Need to be careful with input idx_centroid - you don't want to
    # input idx_centroid that is already positively masked.

    def run_segmentation(
        self,
        n_spots_add_per_step: int = 1,
        coverage_to_stop: float = 0.8,
        max_iter: int = 200,
        verbose: bool = True,
        warnings: bool = False,
        print_summary: bool = True,
    ):
        """Segments the spots into single cells. Seed spots are selected randomly and sequentially.
        Updates self.sampleIds_new, self.confidences_new, self.classes_new."""
        confident_count = 0
        class_count: dict[int, int] = dict()
        for i_iter in range(max_iter):
            if verbose and i_iter % 5 == 0:
                print(f"Iteration {i_iter+1}:")
            available_spots = self.unmasked_spotIds
            if len(available_spots) == 0:
                print("All spots queried. Done.")
                return None
            ix_centroid = _np.random.choice(available_spots)
            if verbose and i_iter % 5 == 0:
                print(f"Querying spot {ix_centroid} ...")
            conf, label, _ = self._buildFiltration_addSpotsUntilConfident(
                idx_centroid=ix_centroid,
                n_spots_add_per_step=n_spots_add_per_step,
                verbose=warnings,
            )
            if conf >= self.threshold_confidence:
                confident_count += 1
                class_count[label] = class_count.get(label, 0) + 1
            if verbose and i_iter % 5 == 0:
                print(
                    f"Spot {ix_centroid} | confidence: {conf:.3e} | confident total: {
                        confident_count} | class: {label}"
                )
                print(f"Classes total: {class_count}")
            coverage = (self.mask_newIds > -1).sum() / len(self.mask_newIds)
            if verbose and i_iter % 5 == 0:
                print(f"Coverage: {coverage*100:.2f}%")
            if coverage >= coverage_to_stop:
                break
        else:
            if warnings:
                print(f"Reaches max_iter {max_iter}!")
        if verbose:
            print("Done.")
        if print_summary:
            print(f"""--- Summary ---
Queried {max_iter} spots (with replacement), of which {confident_count} made up confident single cells.
Classes total (this round): {class_count}
Coverage: {coverage*100:.2f}%
--- --- --- --- ---
""")
        return None

    def run_getSingleCellAnnData(
        self,
        cache: bool = True,
        force: bool = False,
    ) -> _AnnData:
        """Get segmented single-cell level spatial transcriptomic AnnData.
        Note: cache shares the same id with what this method returns."""
        if (not force) and (not (self.cache_singleCellAnnData is _UNDEFINED)):
            return self.cache_singleCellAnnData
        sc_X = []
        raw_X = self.adata_spatial.X.toarray()
        for ix_new in self.sampleIds_new:
            sc_X.append(
                list(raw_X[_np.array(self.filtrations[ix_new]), :].sum(axis=0)))
        sc_adata = _AnnData(
            X=_csr_matrix(sc_X),
            obs=self.adata_spatial.obs.copy().iloc[self.sampleIds_new],
            var=self.adata_spatial.var.copy(),
        )
        sc_adata.obs["confidence"] = 0.0
        for ix_new in self.sampleIds_new:
            sc_adata.obs.loc[str(
                ix_new), "confidence"] = self.confidences_new[ix_new]
        if "cell_type" in sc_adata.obs.columns:
            sc_adata.obs["cell_type_old"] = sc_adata.obs["cell_type"].copy()
        sc_adata.obs["cell_type"] = list(self.classes_new.values())
        # Save cache
        if cache:
            self.cache_singleCellAnnData = sc_adata
        return sc_adata

    def get_spatial_classes(self) -> NDArray[_np.int_]:
        """Get an array of integers, corresponding to class ids of each (old) sample."""
        res = _np.zeros(shape=(self.adata_spatial.shape[0],), dtype=int)
        for i_sample in range(len(res)):
            new_id = self.mask_newIds[i_sample]
            if new_id == -1:
                res[i_sample] = -1
                continue
            new_class = self.classes_new[new_id]
            res[i_sample] = new_class
        return res

    def run_plotClasses(self):
        import seaborn as sns

        spatial_classes = self.get_spatial_classes().astype(str)
        hue_order=_np.sort(_np.unique(spatial_classes))
        if '-1' == hue_order[0]:
            hue_order[:-1] = hue_order[1:]
            hue_order[-1] = '-1'
        return sns.scatterplot(
            x=self.adata_spatial.obs["x"].values,
            y=self.adata_spatial.obs["y"].values,
            hue=spatial_classes,
            hue_order=hue_order
        )

    def run_plotNewIds(self):
        new_ids = self.mask_newIds
        import seaborn as sns

        return sns.scatterplot(
            x=self.adata_spatial.obs["x"].values,
            y=self.adata_spatial.obs["y"].values,
            hue=new_ids.astype(_np.str_),
        )


class SpatialHandlerAutopilot(SpatialHandler):
    """A spatial handler to produce filtrations
    self-guidedly (autopilot), integrate spots into single cells,
    and estimate their confidences.

    Args:
        adata_spatial (AnnData): subcellular spatial transcriptomic
        AnnData, like Stereo-seq, with .obs[['x', 'y']] indicating
        the locations of spots.

        local_classifier (LocalClassifier): a trained local classifier
        on reference scRNA data.

        threshold_adjacent (float): spots within this distance are
        considered adjacent. for integer-indexed spots, 1.2 for
        4-neighbor adjacency and 1.5 for 8-neighbor adjacency.

        max_spots_per_cell (int): max number of spots of a single
        cell.

        scale_rbf (float): next spot to add is selected from adjacent
        spots, with a radial basis function probability, whose scale
        factor is this parameter.

        n_spots_per_unit (int): the number of spots an inference unit
        has. This unit is a small group of cells assumed to be of the
        same type. An inference probability is calculated based on
        this unit of spots for self-guided filtration building.

        multiplier_sameClass (float): the prob of next-to-add spot
        is multiplied by this factor if it is of the same class as the
        center spot (and the probs of all next-spots are re-normalized).
        Expected to be no less than 1. Defaults to 1."""

    def __init__(
        self,
        adata_spatial: _AnnData,
        local_classifier: _LocalClassifier,
        threshold_adjacent: float = 1.2,
        max_spots_per_cell: int = 81,
        scale_rbf: float = 1.0,
        n_spots_per_unit: int = 3,
        multiplier_sameClass: float = 1.0,
    ) -> None:
        assert n_spots_per_unit >= 1
        assert multiplier_sameClass >= 1.0
        super().__init__(
            adata_spatial=adata_spatial,
            local_classifier=local_classifier,
            threshold_adjacent=threshold_adjacent,
            max_spots_per_cell=max_spots_per_cell,
            scale_rbf=scale_rbf,
        )
        self.n_spots_per_unit: int = n_spots_per_unit
        self.multiplier_sameClass: int = multiplier_sameClass
        return None

    def __repr__(self) -> str:
        return f"""--- Spatial Handler Autopilot (pytacs) ---
- adata_spatial: {self.adata_spatial}
- threshold_adjacent: {self.threshold_adjacent}
- local_classifier: {self.local_classifier}
    + threshold_confidence: {self.threshold_confidence}
    + has_negative_control: {self.has_negative_control}
- max_spots_per_cell: {self.max_spots_per_cell}
- scale_rbf: {self.scale_rbf}
- n_spots_per_unit: {self.n_spots_per_unit}
- multiplier_sameClass: {self.multiplier_sameClass}
- filtrations: {len(self.filtrations)} fitted
- single-cell segmentation:
    + new samples: {len(self.sampleIds_new)}
    + AnnData: {self.cache_singleCellAnnData}
--- --- --- --- --- ---
"""

    def _firstRound_preMapping(self) -> None:
        """Updates self.adata_spatial.obs['cell_type_premapping1']
        and .obsm['confidence_premapping1'].

        After the first round mapping, each spot has a confidence.
        But some (or many) spots have rather low confidences due
        to sparsity. They would almost always be left the last ones
        to be added to filtrations preferably by the filtration
        builder when adding next-spots to it, potentially causing
        bias. This could be addressed by performing a second-round
        pre-mapping."""

        # >>> Temporarily change the confidence threshold
        threshold_confidence_old: float = self.threshold_confidence
        self.local_classifier.set_threshold_confidence(0.0)
        confidence_premapping: NDArray[_np.float_] = (
            self.local_classifier.predict_proba(
                X=_to_array(self.adata_spatial.X), genes=self.adata_spatial.var.index
            )
        )
        if self.has_negative_control:
            confidence_premapping = confidence_premapping[:, :-1]
            # only preserves real classes (ids).
        self.adata_spatial.obsm["confidence_premapping1"] = confidence_premapping
        self.adata_spatial.obs["cell_type_premapping1"] = _np.argmax(
            a=confidence_premapping, axis=1
        ).astype(_np.int_)

        # <<< Reset the confidence threshold
        self.local_classifier.set_threshold_confidence(
            value=threshold_confidence_old)
        return None

    def _secondRound_preMapping(self) -> None:
        """Updates self.adata_spatial.obs['cell_type_premapping2']
        and .obsm['confidence_premapping2'].

        After the first round mapping, each spot has a confidence.
        But some (or many) spots have rather low confidences due
        to sparsity. They would almost always be left the last ones
        to be added to filtrations preferably by the filtration
        builder when adding next-spots to it, potentially causing
        bias. This could be addressed by performing a second-round
        pre-mapping.

        The second-round pre-mapping aggregates small amount of
        spots in a local area and assess the confidence of the
        aggregated group of spots as the modified second-round
        confidence of the center spot."""

        self.adata_spatial.obsm["confidence_premapping2"] = _np.zeros(
            shape=self.adata_spatial.obsm["confidence_premapping1"].shape
        )

        for i_spot in range(self.adata_spatial.shape[0]):
            # Get adjacent neighbors
            ixs_adj: NDArray[_np.int_] = super(
            )._find_adjacentOfOneSpot_spotIds(i_spot)
            # Extract confidences
            confidences_adj: NDArray[_np.float_] = self.adata_spatial.obsm[
                "confidence_premapping1"
            ][ixs_adj, :]
            # Extract class
            cls_i: int = self.adata_spatial.obs["cell_type_premapping1"].values[i_spot]
            cls_adj: NDArray[_np.int_] = self.adata_spatial.obs[
                "cell_type_premapping1"
            ].values[ixs_adj]
            # Extract class confidences
            conf_thisCls_adj: NDArray[_np.float_] = confidences_adj[:, cls_i]
            # Apply multiplier
            multipliers: NDArray[_np.float_] = _np.array(
                [
                    self.multiplier_sameClass if cls_thisAdj == cls_i else 1.0
                    for cls_thisAdj in cls_adj
                ]
            )
            conf_thisCls_adj = conf_thisCls_adj * multipliers
            # Normalize probs
            conf_thisCls_adj /= _np.sum(conf_thisCls_adj)
            # Sample another self.n_spots_per_unit-1 spots at probs
            ixs_sampled: NDArray[_np.int_] = _np.unique(
                _np.random.choice(
                    a=ixs_adj,
                    size=self.n_spots_per_unit - 1,
                    replace=True,
                    p=conf_thisCls_adj,
                )
            )
            # Compute confidences of the aggregated spots (unit)
            counts_aggregated: NDArray = _to_array(self.adata_spatial.X)[
                ixs_sampled, :
            ].sum(axis=0)
            counts_aggregated += _to_array(X=self.adata_spatial.X)[i_spot, :]
            pass
